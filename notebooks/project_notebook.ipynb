{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efficient-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from memory_profiler import memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "subsequent-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Lab cell extensions\n",
    "%load_ext rpy2.ipython \n",
    "%load_ext memory_profiler "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-crack",
   "metadata": {},
   "source": [
    "## 3. Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "civilian-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary metadata\n",
    "article_id = 14096681  \n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"figsharedailyrain/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "emerging-perfume",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_link_only': False,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'id': 26579150,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'size': 58863},\n",
       " {'is_link_only': False,\n",
       "  'name': 'environment.yml',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'id': 26579171,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'size': 192},\n",
       " {'is_link_only': False,\n",
       "  'name': 'README.md',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'id': 26586554,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'size': 5422},\n",
       " {'is_link_only': False,\n",
       "  'name': 'data.zip',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'id': 26766812,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'size': 814041183},\n",
       " {'is_link_only': False,\n",
       "  'name': 'get_data.py',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'id': 26766815,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'size': 4113}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  \n",
    "files = data[\"files\"]             \n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "major-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_dl = [\"data.zip\"]\n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "blocked-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-wisconsin",
   "metadata": {},
   "source": [
    "## 4. Combining data CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dominant-series",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "copyrighted-processor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 91.42 MiB, increment: 0.28 MiB\n",
      "Wall time: 6min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "use_cols = [\"time\", \"lat_min\", \"lat_max\", \"lon_min\", \"lon_max\", \"rain (mm/day)\"]\n",
    "files = glob.glob('figsharedailyrain/*.csv')\n",
    "files[:] = [x for x in files if \"observed\" not in x]\n",
    "df = pd.DataFrame(columns=use_cols)\n",
    "df = pd.concat((pd.read_csv(file, index_col=0, usecols=use_cols)\n",
    "                .assign(model=re.findall(r'[^\\/|\\\\]+(?=\\.)', file.replace('_daily_rainfall_NSW', ''))[0])\n",
    "                for file in files)\n",
    "              )  \n",
    "df.to_csv(\"figsharedailyrain/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reasonable-establishment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 152.25 MiB, increment: 0.12 MiB\n",
      "CPU times: user 9min 6s, sys: 16 s, total: 9min 22s\n",
      "Wall time: 9min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "#Chirag's computer (Linux OS)\n",
    "use_cols = [\"time\", \"lat_min\", \"lat_max\", \"lon_min\", \"lon_max\", \"rain (mm/day)\"]\n",
    "files = glob.glob('figsharedailyrain/*.csv')\n",
    "files[:] = [x for x in files if \"observed\" not in x]\n",
    "df = pd.DataFrame(columns=use_cols)\n",
    "df = pd.concat((pd.read_csv(file, index_col=0, usecols=use_cols)\n",
    "                .assign(model=re.findall(r'[^\\/|\\\\]+(?=\\.)', file.replace('_daily_rainfall_NSW', ''))[0])\n",
    "                for file in files)\n",
    "              )  \n",
    "df.to_csv(\"figsharedailyrain/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-tongue",
   "metadata": {},
   "source": [
    "## 5. Load the combined CSV to memory and perform a simple EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-rendering",
   "metadata": {},
   "source": [
    "### 5.1.1 Loading in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interior-adjustment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-CM2           1932840\n",
      "ACCESS-ESM1-5        1610700\n",
      "AWI-ESM-1-1-LR        966420\n",
      "BCC-CSM2-MR          3035340\n",
      "BCC-ESM1              551880\n",
      "CMCC-CM2-HR4         3541230\n",
      "CMCC-CM2-SR5         3541230\n",
      "CMCC-ESM2            3541230\n",
      "CanESM5               551880\n",
      "EC-Earth3-Veg-LR     3037320\n",
      "FGOALS-f3-L          3219300\n",
      "FGOALS-g3            1287720\n",
      "GFDL-CM4             3219300\n",
      "GFDL-ESM4            3219300\n",
      "INM-CM4-8            1609650\n",
      "INM-CM5-0            1609650\n",
      "KIOST-ESM            1287720\n",
      "MIROC6               2070900\n",
      "MPI-ESM-1-2-HAM       966420\n",
      "MPI-ESM1-2-HR        5154240\n",
      "MPI-ESM1-2-LR         966420\n",
      "MRI-ESM2-0           3037320\n",
      "NESM3                 966420\n",
      "NorESM2-LM            919800\n",
      "NorESM2-MM           3541230\n",
      "SAM0-UNICON          3541153\n",
      "TaiESM1              3541230\n",
      "combined_data       54385075\n",
      "dtype: int64\n",
      "peak memory: 9137.07 MiB, increment: 1455.77 MiB\n",
      "CPU times: user 1min 35s, sys: 7.34 s, total: 1min 42s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"figsharedailyrain/combined_data.csv\", chunksize=10_000_000):\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-technical",
   "metadata": {},
   "source": [
    "### 5.1.2 DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "scenic-seattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_data       54385075\n",
      "MPI-ESM1-2-HR        5154240\n",
      "TaiESM1              3541230\n",
      "CMCC-CM2-HR4         3541230\n",
      "CMCC-CM2-SR5         3541230\n",
      "CMCC-ESM2            3541230\n",
      "NorESM2-MM           3541230\n",
      "SAM0-UNICON          3541153\n",
      "FGOALS-f3-L          3219300\n",
      "GFDL-CM4             3219300\n",
      "GFDL-ESM4            3219300\n",
      "EC-Earth3-Veg-LR     3037320\n",
      "MRI-ESM2-0           3037320\n",
      "BCC-CSM2-MR          3035340\n",
      "MIROC6               2070900\n",
      "ACCESS-CM2           1932840\n",
      "ACCESS-ESM1-5        1610700\n",
      "INM-CM5-0            1609650\n",
      "INM-CM4-8            1609650\n",
      "KIOST-ESM            1287720\n",
      "FGOALS-g3            1287720\n",
      "MPI-ESM-1-2-HAM       966420\n",
      "MPI-ESM1-2-LR         966420\n",
      "NESM3                 966420\n",
      "AWI-ESM-1-1-LR        966420\n",
      "NorESM2-LM            919800\n",
      "CanESM5               551880\n",
      "BCC-ESM1              551880\n",
      "Name: model, dtype: int64\n",
      "peak memory: 2396.77 MiB, increment: 1411.14 MiB\n",
      "CPU times: user 2min 31s, sys: 12 s, total: 2min 43s\n",
      "Wall time: 57.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "ddf = dd.read_csv(\"figsharedailyrain/combined_data.csv\")\n",
    "print(ddf[\"model\"].value_counts().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-party",
   "metadata": {},
   "source": [
    "### 5.2 Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-madrid",
   "metadata": {},
   "source": [
    "* We investigated two approaches,first using chunks and second using dask.\n",
    "* The first approach loads the combined csv file in chuncks of `10_000_000` and serially performs EDA of counting models\n",
    "* The second approach using dask internally loads in chunks and performs eda parallely in an optimized way.\n",
    "* The memory usage in both the first and second approach is similar. \n",
    "* The CPU time is higher in the dask approach as compared to first approach. Also there is higher difference between CPU time and Wall time in dask than first approach  \n",
    "* The wall time of second approach using dask is better than the first approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-columbus",
   "metadata": {},
   "source": [
    "## 6. Perform a simple EDA in R\n",
    "\n",
    "Pick an approach to transfer the dataframe from python to R.\n",
    "* Parquet file\n",
    "* Feather file (We chose this one)\n",
    "* Pandas exchange\n",
    "* Arrow exchange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-zimbabwe",
   "metadata": {},
   "source": [
    "### Reasons to choose **feather**\n",
    "\n",
    "The team referred to lecture notes and online resources to compare the four approaches above and determined to choose **Feather** at the end as the best practice. The reasons are listed as follows,\n",
    "\n",
    "- **Feather** is faster than `parquet file` and `arrow exchange` when writing files since it store the data with lesser serialization and deserialization, leading to a higher I/O speed.\n",
    "- `parquet file` has the advantages of saving storage memory. However, it is more appropriate to choose **Feather** as this use case requests a faster speed than data storage.\n",
    "- **Feather** fits well with the R programming language since the API is embedded well for reading and writing data using R.\n",
    "- Team researched online articles to compare those four approaches according to experiments and benchmarking results. The team concluded **Feather** is the ideal choice. \n",
    "- **Feather** works well among Jupyter notebook sessions. It also speeds up the data queries without taking much memory on the disk, and there is no need for any unpacking when loaded the data back into RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-essence",
   "metadata": {},
   "source": [
    "### 6.1 Use Feather to transfer the dataframe from Python to R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import rpy2.rinterface\n",
    "import rpy2_arrow.pyarrow_rarrow as pyra\n",
    "import pyarrow.feather as feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(arrow)\n",
    "library(dplyr)\n",
    "library(tidyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "dataset = ds.dataset(\"figsharedailyrain/combined_data.csv\", format=\"csv\")\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Write in feather format\n",
    "feather.write_feather(table, 'figsharedailyrain/combined_data.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-privacy",
   "metadata": {},
   "source": [
    "### 6.2 Perform a simple EDA in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "\n",
    "# Calculate how much time it took to read a feather file\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_feather(\"figsharedailyrain/combined_data.feather\")\n",
    "print(class(r_table))\n",
    "\n",
    "# Print the different counts of the models \n",
    "result <- r_table %>% count(model) \n",
    "end_time <- Sys.time()\n",
    "\n",
    "print(end_time - start_time)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Print the different counts of the time\n",
    "result <- r_table %>% count(time) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-organ",
   "metadata": {},
   "source": [
    "### 6.3 Discussion\n",
    "\n",
    "There is no difference in terms of the counts for models between the EDA results using Python and R. When comparing the running time, it seems like using R is faster than the speed performed using Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-husband",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "There are several challenges the team faced when dealing with large data sets. We can only perform limited and simple EDA with a local machine since large-scale data is very costive to visualize and calculate using complex matrics and evaluation. It is a bit frustrating to run every cell in the Jupyter notebook in our local machine as it will take a lot of time and might be stuck for a while. Besides, downloading files and deleting them would not be efficient enough as a repeating process to trigger the whole data processing.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
